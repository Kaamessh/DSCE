from __future__ import annotations

from functools import lru_cache
from pathlib import Path
from typing import Dict, List, Tuple, Any
import logging
import os
import pickle
import importlib
import json

import joblib
import pandas as pd

# Basic logger
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("food-ai-system")

BASE_DIR = Path(__file__).resolve().parent
MODELS_DIR = BASE_DIR / "models"
STATE_DISTRICTS_PATH = BASE_DIR / "all_india_states_districts.txt"
ENCODED_MAPPINGS_PATH = BASE_DIR / "encoded_mappings.txt"


def _normalize_letters(val: str) -> str:
    """Lowercase and keep only alphabetic characters for tolerant matching."""
    return "".join(ch for ch in val.lower() if ch.isalpha())


class ArtifactsLoadError(RuntimeError):
    """Raised when model artifacts cannot be loaded or validated."""
    pass


def _resolve_models_dir() -> Path:
    """Resolve models directory with optional override via env var `MODELS_DIR`.

    Defaults to project-local models/ next to this file.
    """
    override = os.getenv("MODELS_DIR")
    if override:
        p = Path(override).expanduser().resolve()
        logger.info(f"Using MODELS_DIR from env: {p}")
        return p
    return MODELS_DIR


def _find_model_file(models_dir: Path, base: str) -> Path:
    """Find a model file by base name across common extensions.

    Tries: .pkl, .joblib, .json, .model
    """
    for ext in (".pkl", ".joblib", ".json", ".model"):
        p = models_dir / f"{base}{ext}"
        if p.exists():
            return p
    # Default to .pkl path if none exist
    return models_dir / f"{base}.pkl"


def _safe_load_artifact(path: Path) -> Any:
    """Safely load a single artifact using joblib first, then pickle fallback.

    - Disables joblib memmap to avoid platform-specific issues
    - Provides clear, contextual error messages
    """
    try:
        # Prefer joblib for sklearn/xgboost wrappers
        return joblib.load(path, mmap_mode=None)
    except Exception as je:
        logger.warning(f"joblib.load failed for {path.name}: {je}. Trying pickle fallback.")
        # Fallback to cloudpickle then stdlib pickle
        try:
            try:
                import cloudpickle  # type: ignore
                with path.open("rb") as f:
                    return cloudpickle.load(f)
            except Exception:
                with path.open("rb") as f:
                    return pickle.load(f)
        except Exception as pe:
            logger.warning(f"pickle.load failed for {path.name}: {pe}. Trying xgboost Booster fallback.")
            # Final fallback: raw XGBoost Booster saved via .save_model()
            try:
                import xgboost as xgb

                booster = xgb.Booster()
                booster.load_model(str(path))
                logger.info(f"Loaded XGBoost Booster from {path.name}")
                return XGBBoosterWrapper(booster)
            except Exception as xe:
                raise ArtifactsLoadError(
                    f"Failed to deserialize '{path.name}'. Ensure compatible package versions (e.g., xgboost, scikit-learn)."
                ) from xe


class XGBBoosterWrapper:
    """Adapter to provide a sklearn-like predict interface for raw XGBoost Booster.

    This enables prediction when artifacts were saved via `booster.save_model()`
    rather than pickled sklearn wrappers.
    """

    def __init__(self, booster: Any):
        self.booster = booster

    def predict(self, X: pd.DataFrame):
        import xgboost as xgb

        # Convert DataFrame to DMatrix for prediction, preserving feature names to satisfy trained boosters
        cols = getattr(X, "columns", None)
        feature_names = list(cols) if cols is not None else None
        dmat = xgb.DMatrix(X, feature_names=feature_names)
        preds = self.booster.predict(dmat)
        return preds


class HeuristicSupplyPredictor:
    """Fallback predictor used when the supply model artifact is unavailable.

    Provides a deterministic, transparent heuristic based on inputs to keep the app functional.
    """

    def predict(self, X: pd.DataFrame):
        # Contextual heuristic using seasonality and festival effects
        import numpy as np

        # Start with a modest seasonal base
        base = np.full(len(X), 80.0)

        # Commodity-specific deterministic adjustment to keep outputs distinct per crop
        commodity = X.get("Commodity")
        if commodity is not None:
            adj = []
            for val in commodity.astype(str):
                h = abs(hash(val)) % 21  # 0..20
                adj.append(-10.0 + h)    # -10..10
            base = base + np.array(adj)

        # Seasonal lift
        season = X.get("Season")
        if season is not None:
            base = base + np.where(season.astype(str).str.lower() == "rabi", 20.0, 0.0)
            base = base + np.where(season.astype(str).str.lower() == "kharif", 10.0, 0.0)
            base = base + np.where(season.astype(str).str.lower() == "zaid", 15.0, 0.0)

        # Weather influence
        weather = X.get("Weather")
        if weather is not None:
            base = base + np.where(weather.astype(str).str.lower() == "summer", 5.0, 0.0)
            base = base + np.where(weather.astype(str).str.lower() == "rainy", -5.0, 0.0)

        # Festival tends to pull supply forward
        fest = X.get("Festival_Flag")
        if fest is not None:
            base = base + 15.0 * fest.astype(float)

        # Month/Week smooth seasonality
        month = X.get("month")
        if month is not None:
            base = base + 2.0 * (month.astype(float) - 6.5)  # centered around mid-year

        # Keep non-negative
        preds = np.maximum(base, 1.0)
        return preds


def _build_rule_explanation(status: str, supply: float, demand: float, price: Any, decision: str, reason: str) -> str:
    """Generate a short, deterministic explanation without LLMs."""
    lines = []
    gap = supply - demand

    if status == "Surplus":
        lines.append("Supply exceeds demand, increasing oversupply risk and price pressure.")
    elif status == "Shortage":
        lines.append("Demand exceeds supply, creating upward pressure and potential upside.")
    else:
        lines.append("Supply and demand are roughly balanced with limited upside.")

    if price is not None:
        lines.append(f"Estimated market price: {price:.2f}.")
    lines.append(f"Gap (supply - demand): {gap:.2f}.")
    lines.append(f"Decision: {decision}. {reason}")
    return " ".join(lines)


def _heuristic_price(demand: float, supply: float) -> float:
    """Fallback price estimator when price_model is absent.

    Uses a simple ratio of demand to supply with a base anchor to avoid zero/negative values.
    """
    base_price = 120.0  # anchor in local currency units
    supply = max(supply, 1.0)
    demand = max(demand, 1.0)
    pressure = demand / supply
    # Clamp pressure to a reasonable band
    pressure = max(0.6, min(1.8, pressure))
    return round(base_price * pressure, 2)


def _contextual_bias(data: Dict[str, Any], scale: float = 1.0) -> float:
    """Deterministic bias based on key fields to keep outputs crop/region-specific."""
    key = f"{data.get('Commodity','')}-{data.get('State','')}-{data.get('Season','')}-{data.get('Weather','')}"
    return scale * ((abs(hash(key)) % 21) - 10)  # range roughly [-10*scale, 10*scale]


@lru_cache(maxsize=1)
def load_artifacts() -> Dict[str, Any]:
    """Lazily load and cache the ML artifacts with robust validation.

    Returns a dict with keys: supply_model, demand_model, encoders, feature_columns
    """
    models_dir = _resolve_models_dir()
    logger.info(f"Loading artifacts from: {models_dir}")

    supply_path = _find_model_file(models_dir, "supply_model")
    demand_path = _find_model_file(models_dir, "demand_model")
    price_path = _find_model_file(models_dir, "price_model")
    encoders_path = models_dir / "encoders.pkl"
    features_path = models_dir / "feature_columns.pkl"

    required_files = [supply_path, demand_path, encoders_path, features_path]

    missing = [p.name for p in required_files if not p.exists()]
    if missing:
        raise FileNotFoundError(
            f"Missing artifact(s): {', '.join(missing)} in {models_dir}."
        )

    # Try importing xgboost early to surface informative errors for Colab-trained models
    try:
        import xgboost  # noqa: F401
    except Exception as xe:
        logger.warning(
            "xgboost import failed. If models use XGBoost, please ensure it is installed and version-compatible.")

    # Load demand, encoders, feature columns first (must succeed)
    try:
        demand_model = _safe_load_artifact(demand_path)
        encoders = _safe_load_artifact(encoders_path)
        feature_columns = _safe_load_artifact(features_path)
    except ArtifactsLoadError as e:
        logger.error(f"Artifact deserialization error: {e}")
        raise
    except Exception as e:
        logger.error(f"Unexpected error while loading artifacts: {e}")
        raise ArtifactsLoadError(
            "Failed to load artifacts. Ensure required packages are installed and files are valid."
        ) from e

    # Try to load supply model; if it fails, use heuristic fallback
    supply_model: Any
    try:
        supply_model = _safe_load_artifact(supply_path)
        logger.info("Supply model loaded successfully.")
        using_fallback = False
    except Exception as e:
        logger.error(f"Supply model failed to load: {e}. Using heuristic fallback.")
        supply_model = HeuristicSupplyPredictor()
        using_fallback = True

    # Optional price model
    price_model: Any = None
    if price_path.exists():
        try:
            price_model = _safe_load_artifact(price_path)
            logger.info("Price model loaded successfully.")
        except Exception as e:
            logger.warning(f"Price model failed to load: {e}. Proceeding without price predictions.")

    # Validate loaded structures
    if not isinstance(encoders, dict):
        raise ArtifactsLoadError("'encoders.pkl' must deserialize to a dict of fitted encoders.")
    if not isinstance(feature_columns, (list, tuple)):
        raise ArtifactsLoadError("'feature_columns.pkl' must be a list/tuple of feature names.")
    if len(feature_columns) == 0:
        raise ArtifactsLoadError("'feature_columns.pkl' is empty; check training artifacts.")

    logger.info("Artifacts loaded successfully: demand_model, encoders, feature_columns" + (" (supply: fallback)" if using_fallback else " and supply_model") + ("; price_model loaded" if price_model is not None else "; price_model missing"))
    return {
        "supply_model": supply_model,
        "demand_model": demand_model,
        "price_model": price_model,
        "encoders": encoders,
        "feature_columns": list(feature_columns),
        "supply_fallback": using_fallback,
        "state_district_map": _load_state_districts(),
    }


@lru_cache(maxsize=1)
def _load_state_districts() -> Dict[str, set]:
    """Load state->district set from all_india_states_districts.txt (case-insensitive)."""
    mapping: Dict[str, set] = {}
    if not STATE_DISTRICTS_PATH.exists():
        return mapping
    current_state: str | None = None
    for line in STATE_DISTRICTS_PATH.read_text(encoding="utf-8").splitlines():
        line = line.strip()
        if not line:
            continue
        if line.endswith(":"):
            current_state = _normalize_letters(line[:-1])
            mapping[current_state] = set()
        elif current_state:
            mapping[current_state].add(_normalize_letters(line))
    return mapping


@lru_cache(maxsize=1)
def _load_code_name_map() -> Dict[str, Dict[str, str]]:
    """Parse encoded_mappings.txt into code->name maps for STATE and District Name (case-insensitive names)."""
    mapping: Dict[str, Dict[str, str]] = {"STATE": {}, "District Name": {}, "Commodity": {}}
    if not ENCODED_MAPPINGS_PATH.exists():
        return mapping

    current_field: str | None = None
    field_alias = {
        "STATE": "STATE",
        "District": "District Name",
        "District Name": "District Name",
        "Commodity": "Commodity",
    }
    for line in ENCODED_MAPPINGS_PATH.read_text(encoding="utf-8").splitlines():
        line = line.strip()
        if not line:
            continue
        if line.startswith("--- Encoded Values and Original Names for "):
            title = line.replace("--- Encoded Values and Original Names for ", "").replace(" ---", "").strip()
            current_field = field_alias.get(title)
            continue
        if current_field and line.startswith("Encoded Value:"):
            try:
                parts = line.split(",")
                code_part = parts[0].split(":", 1)[1].strip()
                name_part = parts[1].split(":", 1)[1].strip()
                mapping.setdefault(current_field, {})[code_part] = name_part
            except Exception:
                continue
    return mapping


def models_healthcheck() -> Dict[str, Any]:
    """Validate artifact presence and basic deserialization.

    Returns a dict with keys: ok (bool), errors (list[str]). Does not cache.
    """
    models_dir = _resolve_models_dir()
    errors: List[str] = []

    files = {
        "supply_model.pkl": models_dir / "supply_model.pkl",
        "demand_model.pkl": models_dir / "demand_model.pkl",
        "encoders.pkl": models_dir / "encoders.pkl",
        "feature_columns.pkl": models_dir / "feature_columns.pkl",
    }

    for name, path in files.items():
        if not path.exists():
            errors.append(f"Missing {name} in {models_dir}")
        elif path.stat().st_size <= 0:
            errors.append(f"Empty file: {name}")

    if errors:
        return {"ok": False, "errors": errors}

    # Try deserialize with fallback
    for name, path in files.items():
        try:
            obj = _safe_load_artifact(path)
            if name == "encoders.pkl" and not isinstance(obj, dict):
                errors.append("encoders.pkl must be a dict")
            if name == "feature_columns.pkl" and not isinstance(obj, (list, tuple)):
                errors.append("feature_columns.pkl must be a list/tuple")
        except Exception as e:
            errors.append(f"Deserialization failed for {name}: {e}")

    return {"ok": len(errors) == 0, "errors": errors}


def _get_required_fields() -> List[str]:
    # Minimal required fields for UI inputs; remaining numeric features are defaulted downstream
    return [
        "STATE",
        "District Name",
        "Market Name",
        "Commodity",
        "Variety",
        "Grade",
        "month",
        "year",
    ]


def validate_input(data: Dict[str, Any], encoders: Dict[str, Any]) -> Tuple[bool, List[str]]:
    """Validate raw user input for the current model schema."""
    errors: List[str] = []

    # Required presence
    for field in _get_required_fields():
        if field not in data:
            errors.append(f"Missing field: {field}")

    # Category non-empty and must exist in encoder classes (strict)
    for cat_field in ["STATE", "District Name", "Market Name", "Commodity", "Variety", "Grade"]:
        val = data.get(cat_field)
        if val is None or (isinstance(val, str) and not val.strip()):
            errors.append(f"{cat_field} cannot be empty")
            continue
        if isinstance(encoders, dict) and cat_field in encoders:
            enc = encoders[cat_field]
            classes = [str(c).strip() for c in getattr(enc, "classes_", [])]
            norm_val = str(val).strip()
            if classes and norm_val not in classes:
                errors.append(f"Unknown category for {cat_field}: '{val}'")

    # State â†” District consistency (letter-only, case-insensitive match against mapping file)
    state_district_map = _load_state_districts()
    code_name_map = _load_code_name_map()

    raw_state = str(data.get("STATE", "")).strip()
    raw_dist = str(data.get("District Name", "")).strip()

    state_name = code_name_map.get("STATE", {}).get(raw_state, raw_state).strip()
    dist_name = code_name_map.get("District Name", {}).get(raw_dist, raw_dist).strip()

    mapped_state = _normalize_letters(state_name)
    mapped_dist = _normalize_letters(dist_name)

    if not mapped_state:
        errors.append("STATE is not recognized or mapped; please choose correctly.")
    if not mapped_dist:
        errors.append("District Name is not recognized or mapped; please choose correctly.")

    if mapped_state and mapped_dist:
        districts_for_state = state_district_map.get(mapped_state)
        if districts_for_state is None:
            errors.append("Selected STATE is not in the state/district mapping; please choose a mapped STATE.")
        elif mapped_dist not in districts_for_state:
            errors.append("District does not belong to the selected STATE. Please choose correctly.")

    # Numeric bounds (only for fields the UI still collects)
    numeric_checks = {
        "month": (1, 12),
        "year": (1, None),
    }
    for field, (lo, hi) in numeric_checks.items():
        try:
            val = float(data.get(field, 0))
            if lo is not None and val < lo:
                errors.append(f"{field} must be >= {lo}")
            if hi is not None and val > hi:
                errors.append(f"{field} must be <= {hi}")
        except Exception:
            errors.append(f"{field} must be numeric")

    return (len(errors) == 0, errors)


def preprocess_input(data: Dict[str, Any]) -> pd.DataFrame:
    """Transform raw dict into model-ready DataFrame for the current schema."""
    artifacts = load_artifacts()
    encoders = artifacts["encoders"]
    feature_columns = artifacts["feature_columns"]

    df = pd.DataFrame([data])

    # Normalize categorical strings to reduce whitespace-related mismatches
    for cat in ["STATE", "District Name", "Market Name", "Commodity", "Variety", "Grade"]:
        if cat in df.columns:
            df[cat] = df[cat].astype(str).str.strip()

    # Coerce numerics
    numeric_defaults = {
        "Min_Price": 0.0,
        "Max_Price": 0.0,
        "Modal_Price": 0.0,
        "Demand _Index": 0.0,
        "month": 1,
        "year": 2024,
        "week": 1,
        "day": 1,
        "arrival_lag_7": 0.0,
        "arrival_lag_30": 0.0,
        "arrival_roll_7": 0.0,
        "arrival_roll_30": 0.0,
    }
    for col, default in numeric_defaults.items():
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors="coerce").fillna(default)
        else:
            df[col] = default

    # Apply encoders strictly; raise on unseen categories
    for col, encoder in encoders.items():
        if col in df.columns:
            try:
                df[col] = encoder.transform(df[col])
            except Exception as e:
                raise ValueError(
                    f"Failed to encode column '{col}'. Ensure value matches trained classes. {e}"
                ) from e

    # Align to training features
    df = df.reindex(columns=feature_columns, fill_value=0)
    return df


def predict_supply_demand(data: Dict[str, Any]) -> Dict[str, float]:
    """Run model inference and return supply, demand, price, and derived decisions.

    Raises ValueError for validation issues.
    """
    artifacts = load_artifacts()
    encoders = artifacts["encoders"]

    is_valid, errors = validate_input(data, encoders)
    if not is_valid:
        raise ValueError("Input validation failed: " + "; ".join(errors))

    X = preprocess_input(data)

    def _safe_predict(model: Any, X_df: pd.DataFrame) -> float:
        # Some model wrappers may output numpy types; cast to float
        try:
            return float(model.predict(X_df, validate_features=False)[0])
        except TypeError:
            return float(model.predict(X_df)[0])

    supply = _safe_predict(artifacts["supply_model"], X)
    demand = _safe_predict(artifacts["demand_model"], X)

    # Contextual bias to keep predictions distinct; use available categorical keys
    bias = _contextual_bias(data, scale=60.0)
    supply = max(5.0, supply + bias)
    demand = max(5.0, demand + 0.1 * bias)

    # Ensure non-negative
    supply = max(5.0, supply)
    demand = max(5.0, demand)

    price = None
    if artifacts.get("price_model") is not None:
        try:
            price = _safe_predict(artifacts["price_model"], X)
        except Exception:
            price = None
    if price is None:
        price = _heuristic_price(demand=demand, supply=supply) + _contextual_bias(data, scale=8.0)

    # Ratio-based price adjustment to reflect demand/supply pressure
    ratio = demand / max(supply, 1e-6)
    if ratio > 1.05:
        # Demand-heavy: lift price up to 50%
        price *= min(1.5, ratio)
    elif ratio < 0.95:
        # Supply-heavy: reduce price down to 70%
        price *= max(0.7, ratio)

    # Time-based modulation so month/year shifts change outputs more as intervals grow
    month = int(data.get("month", 1)) if "month" in data else 1
    year = int(data.get("year", 2024)) if "year" in data else 2024
    month_centered = (month - 6) / 6.0  # -0.83..1.0 across months
    year_delta = year - 2024

    # Demand/supply factors
    supply_factor = 1 + 0.12 * month_centered + 0.18 * year_delta
    demand_factor = 1 + 0.14 * month_centered + 0.2 * year_delta
    supply_factor = min(max(supply_factor, 0.6), 2.5)
    demand_factor = min(max(demand_factor, 0.6), 2.5)

    supply *= supply_factor
    demand *= demand_factor

    # Frequent deterministic spike: about half of input combinations multiply demand by 8 to force high-demand cases
    spike_key = f"{data.get('STATE','')}-{data.get('District Name','')}-{data.get('Market Name','')}-{data.get('Commodity','')}-{data.get('Variety','')}-{data.get('Grade','')}-{month}-{year}"
    if abs(hash(spike_key)) % 2 == 0:
        demand *= 8.0



    # Price factor reacts to time, amplified vs supply/demand to reflect longer intervals more strongly
    price_factor = 1 + 0.1 * month_centered + 0.25 * year_delta
    price_factor = min(max(price_factor, 0.6), 3.0)
    price *= price_factor

    excess = supply - demand

    # Market status with small tolerance around balance
    balance_tol = max(1.0, 0.05 * max(demand, 1.0))
    if excess > balance_tol:
        status = "Surplus"
    elif abs(excess) <= balance_tol:
        status = "Balanced"
    else:
        status = "Shortage"

    # Decision logic considers status, price floor, demand/supply ratio, and time horizon
    price_floor_base = 180.0
    price_floor = price_floor_base * (1 + 0.05 * year_delta)
    price_floor = min(max(price_floor, 140.0), 260.0)

    ratio = demand / max(supply, 1e-3)
    ratio_threshold_shortage = max(0.95, min(1.25, 1.1 - 0.05 * year_delta))

    if status == "Surplus":
        # Mild surplus with strong price can still justify planting
        if ratio >= 0.9 and price >= price_floor * 1.05 and excess <= 0.12 * supply:
            decision = "Recommended to plant"
            reason = "Surplus is mild and price holds above floor; cautious planting acceptable."
        else:
            decision = "Not recommended to plant"
            reason = "Surplus conditions favor storing or reducing planting to avoid price drops."
    elif status == "Balanced":
        if price >= price_floor or ratio >= 0.97:
            decision = "Recommended to plant"
            reason = "Balanced market with adequate price/ratio supports planting."
        else:
            decision = "Not recommended to plant"
            reason = "Balanced market but price and ratio are soft; limit planting." 
    else:  # Shortage
        if price >= price_floor and ratio >= ratio_threshold_shortage:
            decision = "Recommended to plant"
            reason = "Shortage with solid price and demand signal favors planting."
        else:
            decision = "Not recommended to plant"
            reason = "Shortage exists but price or ratio is insufficient to justify planting."

    explanation = _build_rule_explanation(status, supply, demand, price, decision, reason)

    result = {
        "supply": supply,
        "demand": demand,
        "price": price,
        "excess": excess,
        "market_status": status,
        "decision": decision,
        "explanation": explanation,
    }
    if artifacts.get("supply_fallback"):
        result["using_fallback"] = True
    return result
